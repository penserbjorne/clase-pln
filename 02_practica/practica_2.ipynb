{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practica 2\n",
    "\n",
    "**Objetivo:** A partir del [corpus proporcionado](./../99_corpus/corpusML.txt) realizar un modelo del lenguaje neuronal con base en la arquitectura propuesta por Bengio (2003)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from re import sub\n",
    "from unicodedata import normalize\n",
    "from nltk.stem.snowball import SpanishStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from itertools import chain\n",
    "from collections import Counter,defaultdict\n",
    "\n",
    "#import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "#from scipy.optimize import minimize as min\n",
    "\n",
    "def ejemplos(msg, collection, n_elements):\n",
    "    print(msg)\n",
    "    for element in collection[:n_elements]:\n",
    "        print(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Limpiar los textos y aplicar stemming a las palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abrir el documento\n",
    "document = './../99_corpus/corpusML.txt'\n",
    "text = open(document,'r',encoding='utf-8').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Lineas de ejemplo ###\n",
      "\n",
      "comencé a trabajar y me pegaron me maltrataron con chicote\n",
      "mis patrones me pegaron porque no me quería apurar porque era flojo\n",
      "por eso me habían pegado\n",
      "cuando me pegaban ya entonces me quitaba\n",
      "pues entonces no quise trabajar\n"
     ]
    }
   ],
   "source": [
    "# Limpiamos el documento\n",
    "text_clean = \"\"\n",
    "lines = []\n",
    "\n",
    "for line in text.split(\"\\n\"):\n",
    "    # Eliminamos caracteres compuestos y pasamos a minusculas\n",
    "    line = normalize('NFKC', line).lower()\n",
    "    # Eliminamos extensiones y numeros\n",
    "    line = sub(r\"\\\\.*|{.*}|\\\\|\\[.*\\]|[!-@[-`{-~]\", ' ', line)\n",
    "    # Eliminamos signos de puntuacion\n",
    "    line = sub(r\"[^\\w]\", \" \", line)\n",
    "    # Eliminamos saltos de linea\n",
    "    line = \" \".join(line.split())\n",
    "    # Si la linea no esta vacia la añadimos al texto limpio\n",
    "    if line:\n",
    "        lines.append(line)\n",
    "        \n",
    "text_clean += \" \".join(lines)\n",
    "text_clean = text_clean.split()\n",
    "\n",
    "ejemplos(\"### Lineas de ejemplo ###\\n\", lines, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Lineas de ejemplo ###\n",
      "\n",
      "['comenc', 'a', 'trabaj', 'y', 'me', 'peg', 'me', 'maltrat', 'con', 'chicot']\n",
      "['mis', 'patron', 'me', 'peg', 'porqu', 'no', 'me', 'quer', 'apur', 'porqu', 'era', 'floj']\n",
      "['por', 'eso', 'me', 'hab', 'peg']\n",
      "['cuand', 'me', 'peg', 'ya', 'entonc', 'me', 'quit']\n",
      "['pues', 'entonc', 'no', 'quis', 'trabaj']\n"
     ]
    }
   ],
   "source": [
    "# Aplicamos Stemming a los tokens limpios\n",
    "\n",
    "# Debido a que el corpus se encuentra en español, utilizaremos el stemmer de NLTK para español.\n",
    "stemmer = SpanishStemmer()\n",
    "stems = []\n",
    "for line in lines:\n",
    "    stems.append([stemmer.stem(word) for word in line.split(\" \")])\n",
    "    \n",
    "ejemplos(\"### Lineas de ejemplo ###\\n\", stems, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Corpus de entrenamiento ###\n",
      "\n",
      "Tamaño del corpus de entrenamiento: 751  lineas\n",
      "\n",
      "Lineas de ejemplo\n",
      "['pues', 'lo', 'pag', 'y', 'estuv', 'bien', 'entonc']\n",
      "['voy', 'a', 'cuid', 'mi', 'poll']\n",
      "['mis', 'hoj', 'de', 'maiz', 'todav', 'no', 'me', 'las', 'acab', 'nomas', 'estan']\n",
      "['dijeron', 'dond', 'ha', 'estad', 'nuestr', 'madr', 'por', 'mas', 'de', 'eso']\n",
      "['se', 'las', 'llev', 'con', 'burrit', 'nomas']\n",
      "\n",
      "### Corpus de evaluacion ###\n",
      "\n",
      "Tamaño del corpus de evaluación: 323  lineas\n",
      "\n",
      "Lineas de ejemplo\n",
      "['per', 'el', 'niñ', 'se', 'alegr']\n",
      "['me', 'iba', 'a', 'andar', 'por', 'alli', 'hast', 'la', 'tard']\n",
      "['pues', 'si', 'pues', 'no', 'per', 'si', 'nos', 'regañ']\n",
      "['lueg', 'este', 'com', 'hab', 'trabaj', 'que', 'entonc', 'trabaj', 'el', 'hombr']\n",
      "['no', 'vim', 'ningun', 'de', 'esas', 'que', 'pic', 'fin', 'porqu', 'estab', 'sec']\n"
     ]
    }
   ],
   "source": [
    "# Separación del corpus en prueba (70%) y evaluación (30%)\n",
    "train_corpus, eval_corpus = train_test_split(stems, test_size=0.3)\n",
    "\n",
    "print(\"### Corpus de entrenamiento ###\")\n",
    "print(\"\\nTamaño del corpus de entrenamiento:\",len(train_corpus), \" lineas\")\n",
    "ejemplos(\"\\nLineas de ejemplo\", train_corpus, 5)\n",
    "\n",
    "print(\"\\n### Corpus de evaluacion ###\")\n",
    "print(\"\\nTamaño del corpus de evaluación:\", len(eval_corpus), \" lineas\")\n",
    "ejemplos(\"\\nLineas de ejemplo\", eval_corpus, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Insertar símbolos de inicio y final de cadena."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos los simbolos\n",
    "BOS = '<BOS>'\n",
    "EOS = '<EOS>'\n",
    "UNK = '<UNK>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definamos una función para sustituir los hapax, esta funcion recibe un corpus\n",
    "def replace_hapax(corpus):\n",
    "    # Obtenemos las frecuencias de las palabras del corpus\n",
    "    freqs = Counter( chain(*[' '.join(sent).split() for sent in corpus]) )\n",
    "\n",
    "    # Sustituimos los hápax por UNK en las palabras con frecuencia = 1\n",
    "    corpus_unk = []\n",
    "\n",
    "    for line in corpus:\n",
    "        new_line = []\n",
    "        for word in line:\n",
    "            # Si la frecuencia de la palabra = 1, es un hapáx\n",
    "            if freqs[word] == 1:\n",
    "                new_line.append(UNK)\n",
    "            # Si no, añadimos la palabra original\n",
    "            else:\n",
    "                new_line.append(word)\n",
    "        corpus_unk.append(new_line)\n",
    "    return corpus_unk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Corpus de entrenamiento ###\n",
      "\n",
      "Tamaño del corpus de entrenamiento: 751  lineas\n",
      "\n",
      "Lineas de ejemplo del corpus de entrenamiento\n",
      "['pues', 'lo', 'pag', 'y', 'estuv', 'bien', 'entonc']\n",
      "['voy', 'a', 'cuid', 'mi', 'poll']\n",
      "['mis', 'hoj', 'de', 'maiz', 'todav', 'no', 'me', 'las', 'acab', 'nomas', 'estan']\n",
      "['<UNK>', 'dond', 'ha', 'estad', 'nuestr', 'madr', 'por', 'mas', 'de', 'eso']\n",
      "['se', 'las', 'llev', 'con', 'burrit', 'nomas']\n",
      "\n",
      "### Corpus de evaluacion ###\n",
      "\n",
      "Tamaño del corpus de evaluación: 323  lineas\n",
      "\n",
      "Lineas de ejemplo del corpus de evaluacion\n",
      "['per', 'el', 'niñ', 'se', '<UNK>']\n",
      "['me', 'iba', 'a', '<UNK>', 'por', 'alli', 'hast', 'la', 'tard']\n",
      "['pues', 'si', 'pues', 'no', 'per', 'si', 'nos', 'regañ']\n",
      "['lueg', 'este', 'com', 'hab', 'trabaj', 'que', 'entonc', 'trabaj', 'el', 'hombr']\n",
      "['no', '<UNK>', '<UNK>', 'de', 'esas', 'que', 'pic', '<UNK>', 'porqu', 'estab', '<UNK>']\n"
     ]
    }
   ],
   "source": [
    "# Sustituimos los hápax en los corpus\n",
    "\n",
    "train_corpus_unk = []\n",
    "train_corpus_unk = replace_hapax(train_corpus)\n",
    "print(\"### Corpus de entrenamiento ###\")\n",
    "print(\"\\nTamaño del corpus de entrenamiento:\",len(train_corpus_unk), \" lineas\")\n",
    "ejemplos(\"\\nLineas de ejemplo del corpus de entrenamiento\", train_corpus_unk, 5)\n",
    "\n",
    "eval_corpus_unk = []\n",
    "eval_corpus_unk = replace_hapax(eval_corpus)\n",
    "print(\"\\n### Corpus de evaluacion ###\")\n",
    "print(\"\\nTamaño del corpus de evaluación:\", len(eval_corpus_unk), \" lineas\")\n",
    "ejemplos(\"\\nLineas de ejemplo del corpus de evaluacion\", eval_corpus_unk, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definamos una función para insertar los BOS y EOS\n",
    "def insert_simbols_of_sentence(corpus):\n",
    "    corpus_w_simbols = [ [BOS] + line + [EOS] for line in corpus ]\n",
    "    return corpus_w_simbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Corpus de entrenamiento ###\n",
      "\n",
      "Tamaño del corpus de entrenamiento: 751  lineas\n",
      "\n",
      "Lineas de ejemplo del corpus de entrenamiento\n",
      "['<BOS>', 'pues', 'lo', 'pag', 'y', 'estuv', 'bien', 'entonc', '<EOS>']\n",
      "['<BOS>', 'voy', 'a', 'cuid', 'mi', 'poll', '<EOS>']\n",
      "['<BOS>', 'mis', 'hoj', 'de', 'maiz', 'todav', 'no', 'me', 'las', 'acab', 'nomas', 'estan', '<EOS>']\n",
      "['<BOS>', '<UNK>', 'dond', 'ha', 'estad', 'nuestr', 'madr', 'por', 'mas', 'de', 'eso', '<EOS>']\n",
      "['<BOS>', 'se', 'las', 'llev', 'con', 'burrit', 'nomas', '<EOS>']\n",
      "\n",
      "### Corpus de evaluacion ###\n",
      "\n",
      "Tamaño del corpus de evaluacion: 323  lineas\n",
      "\n",
      "Lineas de ejemplo del corpus de evaluacion\n",
      "['<BOS>', 'per', 'el', 'niñ', 'se', '<UNK>', '<EOS>']\n",
      "['<BOS>', 'me', 'iba', 'a', '<UNK>', 'por', 'alli', 'hast', 'la', 'tard', '<EOS>']\n",
      "['<BOS>', 'pues', 'si', 'pues', 'no', 'per', 'si', 'nos', 'regañ', '<EOS>']\n",
      "['<BOS>', 'lueg', 'este', 'com', 'hab', 'trabaj', 'que', 'entonc', 'trabaj', 'el', 'hombr', '<EOS>']\n",
      "['<BOS>', 'no', '<UNK>', '<UNK>', 'de', 'esas', 'que', 'pic', '<UNK>', 'porqu', 'estab', '<UNK>', '<EOS>']\n"
     ]
    }
   ],
   "source": [
    "# Insertamos BOS y EOS\n",
    "train_corpus_w_simbols = []\n",
    "train_corpus_w_simbols = insert_simbols_of_sentence(train_corpus_unk)\n",
    "print(\"### Corpus de entrenamiento ###\")\n",
    "print(\"\\nTamaño del corpus de entrenamiento:\",len(train_corpus_w_simbols), \" lineas\")\n",
    "ejemplos(\"\\nLineas de ejemplo del corpus de entrenamiento\", train_corpus_w_simbols, 5)\n",
    "\n",
    "eval_corpus_w_simbols = []\n",
    "eval_corpus_w_simbols = insert_simbols_of_sentence(eval_corpus_unk)\n",
    "print(\"\\n### Corpus de evaluacion ###\")\n",
    "print(\"\\nTamaño del corpus de evaluacion:\",len(eval_corpus_w_simbols), \" lineas\")\n",
    "ejemplos(\"\\nLineas de ejemplo del corpus de evaluacion\", eval_corpus_w_simbols, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Obtener los bigramas que aparecen en el texto (indexar numéricamente)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definamos una función para indexar numericamente\n",
    "def corpus2index(corpus):\n",
    "    # Creamos un diccionario para los stems\n",
    "    vocab = defaultdict()\n",
    "    vocab.default_factory = lambda: len(vocab)\n",
    "\n",
    "    # Creamos el corpus con sus indices\n",
    "    corpus_ids = [[vocab[word] for word in line] for line in corpus]\n",
    "    return corpus_ids, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Corpus de entrenamiento ###\n",
      "\n",
      "Oraciones en corpus de entrenamiento\n",
      "['<BOS>', 'pues', 'lo', 'pag', 'y', 'estuv', 'bien', 'entonc', '<EOS>']\n",
      "['<BOS>', 'voy', 'a', 'cuid', 'mi', 'poll', '<EOS>']\n",
      "['<BOS>', 'mis', 'hoj', 'de', 'maiz', 'todav', 'no', 'me', 'las', 'acab', 'nomas', 'estan', '<EOS>']\n",
      "['<BOS>', '<UNK>', 'dond', 'ha', 'estad', 'nuestr', 'madr', 'por', 'mas', 'de', 'eso', '<EOS>']\n",
      "['<BOS>', 'se', 'las', 'llev', 'con', 'burrit', 'nomas', '<EOS>']\n",
      "\n",
      "Indices en corpus de entrenamiento\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "[0, 9, 10, 11, 12, 13, 8]\n",
      "[0, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 8]\n",
      "[0, 25, 26, 27, 28, 29, 30, 31, 32, 16, 33, 8]\n",
      "[0, 34, 21, 35, 36, 37, 23, 8]\n",
      "\n",
      "### Corpus de evaluacion ###\n",
      "\n",
      "Oraciones en corpus de evaluacion\n",
      "['<BOS>', 'per', 'el', 'niñ', 'se', '<UNK>', '<EOS>']\n",
      "['<BOS>', 'me', 'iba', 'a', '<UNK>', 'por', 'alli', 'hast', 'la', 'tard', '<EOS>']\n",
      "['<BOS>', 'pues', 'si', 'pues', 'no', 'per', 'si', 'nos', 'regañ', '<EOS>']\n",
      "['<BOS>', 'lueg', 'este', 'com', 'hab', 'trabaj', 'que', 'entonc', 'trabaj', 'el', 'hombr', '<EOS>']\n",
      "['<BOS>', 'no', '<UNK>', '<UNK>', 'de', 'esas', 'que', 'pic', '<UNK>', 'porqu', 'estab', '<UNK>', '<EOS>']\n",
      "\n",
      "Indices en corpus de evaluacion\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "[0, 7, 8, 9, 5, 10, 11, 12, 13, 14, 6]\n",
      "[0, 15, 16, 15, 17, 1, 16, 18, 19, 6]\n",
      "[0, 20, 21, 22, 23, 24, 25, 26, 24, 2, 27, 6]\n",
      "[0, 17, 5, 5, 28, 29, 25, 30, 5, 31, 32, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "# Indexamos numericamente los stems\n",
    "\n",
    "train_corpus_ids, train_vocab = corpus2index(train_corpus_w_simbols)\n",
    "print(\"### Corpus de entrenamiento ###\")\n",
    "ejemplos(\"\\nOraciones en corpus de entrenamiento\", train_corpus_w_simbols, 5)\n",
    "ejemplos(\"\\nIndices en corpus de entrenamiento\", train_corpus_ids, 5)\n",
    "\n",
    "eval_corpus_ids, eval_vocab = corpus2index(eval_corpus_w_simbols)\n",
    "print(\"\\n### Corpus de evaluacion ###\")\n",
    "ejemplos(\"\\nOraciones en corpus de evaluacion\", eval_corpus_w_simbols, 5)\n",
    "ejemplos(\"\\nIndices en corpus de evaluacion\", eval_corpus_ids, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definamos una función para crear los bigrama\n",
    "def corpus2bigrams(corpus):\n",
    "    bigrams = list(chain(*[zip(cad,cad[1:]) for cad in corpus]))\n",
    "    return bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Corpus de entrenamiento ###\n",
      "\n",
      "Número de bigramas: 6966\n",
      "\n",
      "Ejemplos de bigramas de entrenamiento\n",
      " [(0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 7), (7, 8), (0, 9), (9, 10), (10, 11), (11, 12), (12, 13), (13, 8), (0, 14), (14, 15), (15, 16), (16, 17), (17, 18), (18, 19), (19, 20), (20, 21), (21, 22), (22, 23), (23, 24), (24, 8), (0, 25), (25, 26), (26, 27), (27, 28), (28, 29), (29, 30), (30, 31), (31, 32), (32, 16), (16, 33), (33, 8), (0, 34), (34, 21), (21, 35), (35, 36), (36, 37), (37, 23), (23, 8), (0, 38), (38, 39), (39, 1), (1, 34), (34, 40), (40, 8)]\n",
      "\n",
      "### Corpus de evaluacion ###\n",
      "\n",
      "Número de bigramas: 2906\n",
      "\n",
      "Ejemplos de bigramas de evaluacion\n",
      " [(0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 7), (7, 8), (0, 9), (9, 10), (10, 11), (11, 12), (12, 13), (13, 8), (0, 14), (14, 15), (15, 16), (16, 17), (17, 18), (18, 19), (19, 20), (20, 21), (21, 22), (22, 23), (23, 24), (24, 8), (0, 25), (25, 26), (26, 27), (27, 28), (28, 29), (29, 30), (30, 31), (31, 32), (32, 16), (16, 33), (33, 8), (0, 34), (34, 21), (21, 35), (35, 36), (36, 37), (37, 23), (23, 8), (0, 38), (38, 39), (39, 1), (1, 34), (34, 40), (40, 8)]\n"
     ]
    }
   ],
   "source": [
    "# Obtencion de los bigramas\n",
    "train_bigrams = []\n",
    "train_bigrams = corpus2bigrams(train_corpus_ids)\n",
    "print(\"### Corpus de entrenamiento ###\")\n",
    "print(\"\\nNúmero de bigramas:\", len(train_bigrams))\n",
    "print(\"\\nEjemplos de bigramas de entrenamiento\\n\", train_bigrams[:50])\n",
    "\n",
    "eval_bigrams = []\n",
    "eval_bigrams = corpus2bigrams(eval_corpus_ids)\n",
    "print(\"\\n### Corpus de evaluacion ###\")\n",
    "print(\"\\nNúmero de bigramas:\", len(eval_bigrams))\n",
    "print(\"\\nEjemplos de bigramas de evaluacion\\n\", train_bigrams[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Entrenar con los bigramas la red neuronal y obtener los valores para los hiperparámetros. Tomar de 100 unidades para la primera capa oculta (capa lineal) y 300 para la segunda capa oculta (capa con tanh).`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos hiperparametros de la red\n",
    "T = 100    # Numero de iteraciones\n",
    "d = 100    # Dimension de los embeddings\n",
    "m = 300    # Numero de unidades en la capa oculta\n",
    "n = 0.1    # Rango de aprendizaje\n",
    "N = len(train_vocab) # Tamaño del vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Obtener las matrices A y Π a partir de las salidas de la red neuronal (probabilidad Softmax)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo de lenguaje μ = (Σ,A,Π), \n",
    "# Σ es un conjunto de símbolos o vocabulario\n",
    "# A = (ai j) = p(wj|wi) son las probabilidades de transicion de los bigramas\n",
    "# Π = (πi) = p(wi) son las probabilidades iniciales.\n",
    "# Para determinar la probabilidad de cualquier cadena en un lenguaje\n",
    "# cuyo vocabulario sea Σ se requiere unicamente conocer N2 \n",
    "# probabilidades de transicion y N probabilidades iniciales, donde\n",
    "# |Σ| = N es el tamano del vocabulario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluar el modelo (con Entropía)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Calcular la probabilidad de las siguientes oraciones:\n",
    "   - Nos bañamos con agua caliente\n",
    "   - El animalito le olía la cabeza\n",
    "   - Pascuala ordeñaba las vacas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
